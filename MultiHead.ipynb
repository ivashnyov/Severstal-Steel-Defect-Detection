{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from factory import *\n",
    "import torch\n",
    "import os \n",
    "import sys\n",
    "from catalyst.dl.callbacks import CriterionCallback, EarlyStoppingCallback, OptimizerCallback, CriterionAggregatorCallback, F1ScoreCallback, AUCCallback\n",
    "from catalyst.dl.runner import SupervisedRunner\n",
    "from pytorch_toolbelt import losses as L\n",
    "from pytorch_toolbelt.inference import tta\n",
    "import collections\n",
    "from pytorch_toolbelt.utils.catalyst import * \n",
    "from metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "from viz_utils import *\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "#import segmentation_models_pytorch as smp\n",
    "from catalyst.contrib.optimizers import RAdam, Lookahead\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --force-reinstall catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_name = 'resnet34' #also densenet169, resnet50, se_resnet50\n",
    "sample_submission_path = 'data/sample_submission.csv'\n",
    "train_df_path = 'data/train.csv'\n",
    "data_folder = \"data/train_images/\"\n",
    "test_data_folder = \"data/test_images/\"\n",
    "base_exp_name = '{}_multihead'.format(encoder_name)\n",
    "log_dir = 'logs/{}/'.format(base_exp_name)\n",
    "batch_size = 16\n",
    "batch_size_val = 8\n",
    "accumulation_steps = 5\n",
    "num_workers = 24\n",
    "num_epochs_with_frozen_encoder = 5\n",
    "num_epochs = 60\n",
    "tta_type = None\n",
    "output_channels = 4\n",
    "output_channels_class = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r logs/resnet34_multihead/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_boundary_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    dilated = binary_dilation(mask, structure=np.ones((9, 9), dtype=np.bool))\n",
    "    dilated = binary_fill_holes(dilated)\n",
    "    diff = dilated & ~mask\n",
    "    diff = cv2.dilate(diff, kernel=(9, 9))\n",
    "    diff = diff & ~mask\n",
    "    #kernel = np.ones((4,),np.uint8)\n",
    "    #diff = cv2.morphologyEx(mask, cv2.MORPH_GRADIENT, kernel)\n",
    "    return diff.astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "def make_mask(row_id, df):\n",
    "    '''Given a row index, return image_id and mask (256, 1600, 4) from the dataframe `df`'''\n",
    "    fname = df.iloc[row_id].name\n",
    "    labels = df.iloc[row_id][:4]\n",
    "    masks = np.zeros((256, 1600, 4), dtype=np.float32) # float32 is V.Imp\n",
    "    # 4:class 1～4 (ch:0～3)\n",
    "\n",
    "    for idx, label in enumerate(labels.values):\n",
    "        if label is not np.nan:\n",
    "            label = label.split(\" \")\n",
    "            positions = map(int, label[0::2])\n",
    "            length = map(int, label[1::2])\n",
    "            mask = np.zeros(256 * 1600, dtype=np.uint8)\n",
    "            for pos, le in zip(positions, length):\n",
    "                mask[pos:(pos + le)] = 1\n",
    "            masks[:, :, idx] = mask.reshape(256, 1600, order='F')\n",
    "    return fname, masks\n",
    "\n",
    "\n",
    "class SteelDatasetMultiV3(Dataset):\n",
    "    def __init__(self, df, data_folder, transforms, phase, prepare_coarse = False, prepare_edges = False, prepare_class = False, prepare_full = False):\n",
    "        self.df = df\n",
    "        self.root = data_folder\n",
    "        self.phase = phase\n",
    "        self.transforms = transforms\n",
    "        self.fnames = self.df.index.tolist()\n",
    "        self.prepare_coarse = prepare_coarse\n",
    "        self.prepare_edges = prepare_edges\n",
    "        self.prepare_class = prepare_class\n",
    "        self.prepare_full = prepare_full\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_id, mask = make_mask(idx, self.df)\n",
    "        image_path = os.path.join(self.root,  image_id)\n",
    "        img = cv2.imread(image_path)\n",
    "        if self.transforms is not None:\n",
    "            augmented = self.transforms(image=img, mask=mask)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask'].astype(np.uint8)\n",
    "        if self.prepare_full:\n",
    "            all_masks_combined = (mask.sum(axis=2)>0).astype(np.uint8)        \n",
    "            all_masks_combined = np.expand_dims(all_masks_combined, 2)\n",
    "            mask = np.concatenate([mask, all_masks_combined], axis=2)\n",
    "        if self.prepare_coarse:\n",
    "            coarse_mask = cv2.resize(mask,\n",
    "                                     dsize=(mask.shape[1]//4, mask.shape[0]//4),\n",
    "                                     interpolation=cv2.INTER_LINEAR)\n",
    "        if self.prepare_edges:\n",
    "            all_masks_combined = (mask.sum(axis=2)>0).astype(np.uint8)  \n",
    "            edges = compute_boundary_mask(all_masks_combined).astype(np.uint8)\n",
    "            edges = np.expand_dims(edges, 2)\n",
    "            mask = np.concatenate([mask, edges], axis=2)\n",
    "            if self.prepare_coarse:\n",
    "                coarse_edges = cv2.resize(edges,\n",
    "                                     dsize=(mask.shape[1]//4, mask.shape[0]//4),\n",
    "                                     interpolation=cv2.INTER_LINEAR)\n",
    "                coarse_edges = np.expand_dims(coarse_edges, 2)\n",
    "                coarse_mask = np.concatenate([coarse_mask, coarse_edges], axis=2)\n",
    "                \n",
    "        if self.prepare_class:\n",
    "            defects =  (np.array([mask[:,:,plane].sum()>0 for plane in range(mask.shape[2])])).astype(int)\n",
    "            \n",
    "        data = {'features': tensor_from_rgb_image(img),\n",
    "                'targets' : tensor_from_mask_image(mask).float(),\n",
    "                'image_id' : image_id}\n",
    "        if self.prepare_coarse:\n",
    "            data['coarse_targets'] =  tensor_from_mask_image(coarse_mask).float()  \n",
    "        if self.prepare_class:\n",
    "            data['classification_labels'] = defects.astype(float)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)    \n",
    "    \n",
    "def provider(\n",
    "    data_folder,\n",
    "    df_path,\n",
    "    phase,\n",
    "    transforms,    \n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "    prepare_coarse = False, \n",
    "    prepare_edges = False,\n",
    "    prepare_class = False, \n",
    "    prepare_full = False\n",
    "):\n",
    "    '''Returns dataloader for the model training'''\n",
    "    df = pd.read_csv(df_path)\n",
    "    # https://www.kaggle.com/amanooo/defect-detection-starter-u-net\n",
    "    df['ImageId'], df['ClassId'] = zip(*df['ImageId_ClassId'].str.split('_'))\n",
    "    df['ClassId'] = df['ClassId'].astype(int)\n",
    "    df = df.pivot(index='ImageId',columns='ClassId',values='EncodedPixels')\n",
    "    df['defects'] = df.count(axis=1)\n",
    "\n",
    "    \n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"defects\"], random_state=69)\n",
    "    df = train_df if phase == \"train\" else val_df\n",
    "    image_dataset = SteelDatasetMultiV3(df, data_folder, \n",
    "                                        transforms, phase, \n",
    "                                        prepare_coarse, prepare_edges, \n",
    "                                        prepare_class, prepare_full)\n",
    "    if phase=='train':\n",
    "        shuffle = True\n",
    "    else:\n",
    "        shuffle = False\n",
    "    dataloader = DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=shuffle,   \n",
    "    )\n",
    "\n",
    "    return dataloader    \n",
    "\n",
    "def light_augmentations():\n",
    "    return A.Compose([\n",
    "\n",
    "        # D4 Augmentations\n",
    "        A.Compose([\n",
    "            #A.Transpose(),            \n",
    "            A.HorizontalFlip(),            \n",
    "            A.VerticalFlip()\n",
    "            #A.RandomRotate90(),\n",
    "        ]),\n",
    "\n",
    "        # Spatial-preserving augmentations:\n",
    "        A.OneOf([\n",
    "            A.Cutout(),\n",
    "            A.GaussNoise(),\n",
    "        ]),\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(),\n",
    "            A.CLAHE(),\n",
    "            A.HueSaturationValue(),\n",
    "            A.RGBShift(),\n",
    "            A.RandomGamma()\n",
    "        ]),\n",
    "\n",
    "        A.Normalize()\n",
    "    ])\n",
    "\n",
    "\n",
    "def medium_augmentations():\n",
    "    return A.Compose([\n",
    "\n",
    "        # Add occasion blur/sharpening\n",
    "        A.OneOf([\n",
    "            A.GaussianBlur(),\n",
    "            A.MotionBlur(),\n",
    "            A.IAASharpen()\n",
    "        ]),\n",
    "\n",
    "        # D4 Augmentations\n",
    "        A.Compose([\n",
    "            #A.Transpose(),\n",
    "            A.VerticalFlip(),\n",
    "            A.HorizontalFlip()\n",
    "            #A.RandomRotate90(),\n",
    "        ]),\n",
    "\n",
    "        # Spatial-preserving augmentations:\n",
    "        A.OneOf([\n",
    "            A.Cutout(),\n",
    "            A.GaussNoise(),\n",
    "        ]),\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(),\n",
    "            A.CLAHE(),\n",
    "            A.HueSaturationValue(),\n",
    "            A.RGBShift(),\n",
    "            A.RandomGamma()\n",
    "        ]),\n",
    "\n",
    "        A.Normalize()\n",
    "    ])\n",
    "\n",
    "\n",
    "def hard_augmentations():\n",
    "    return A.Compose([\n",
    "\n",
    "        # Add occasion blur\n",
    "        A.OneOf([\n",
    "            A.GaussianBlur(),\n",
    "            A.GaussNoise(),\n",
    "            A.IAAAdditiveGaussianNoise(),\n",
    "            A.NoOp()\n",
    "        ]),\n",
    "\n",
    "        # D4 Augmentations\n",
    "        A.Compose([\n",
    "            #A.Transpose(),\n",
    "            A.VerticalFlip(),\n",
    "            A.HorizontalFlip()\n",
    "            #A.RandomRotate90(),\n",
    "        ]),\n",
    "\n",
    "        A.Cutout(),\n",
    "        # Spatial-preserving augmentations:\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(),\n",
    "            A.CLAHE(),\n",
    "            A.HueSaturationValue(),\n",
    "            A.RGBShift(),\n",
    "            A.RandomGamma(),\n",
    "            A.NoOp()\n",
    "        ]),\n",
    "\n",
    "        A.Normalize()\n",
    "    ])\n",
    "\n",
    "\n",
    "def validation_augmentations():\n",
    "    return A.Compose([\n",
    "        A.Normalize()\n",
    "    ],p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_path = './segmentation_models.pytorch/'\n",
    "sys.path.append(package_path)\n",
    "import segmentation_models_pytorch_local as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = provider(\n",
    "    data_folder=data_folder,\n",
    "    df_path=train_df_path,\n",
    "    phase='train',\n",
    "    transforms=light_augmentations(),\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers, \n",
    "    prepare_coarse=True, \n",
    "    prepare_edges=False, \n",
    "    prepare_class=True,\n",
    "    prepare_full=False)\n",
    "dataloader_val = provider(\n",
    "    data_folder=data_folder,\n",
    "    df_path=train_df_path,\n",
    "    phase='val',\n",
    "    transforms=validation_augmentations(),\n",
    "    batch_size=batch_size_val,\n",
    "    num_workers=num_workers, \n",
    "    prepare_coarse=True, \n",
    "    prepare_edges=False, \n",
    "    prepare_class=True,\n",
    "    prepare_full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b = next(iter(dataloader_train))\n",
    "print(data_b.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2,2)\n",
    "idx = 0\n",
    "ax[0,0].imshow(data_b['targets'][idx][0,:,:].numpy())\n",
    "ax[0,1].imshow(data_b['targets'][idx][1,:,:].numpy())\n",
    "ax[1,0].imshow(data_b['targets'][idx][2,:,:].numpy())\n",
    "ax[1,1].imshow(data_b['targets'][idx][3,:,:].numpy())\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2,2)\n",
    "idx = 0\n",
    "ax[0,0].imshow(data_b['coarse_targets'][idx][0,:,:].numpy())\n",
    "ax[0,1].imshow(data_b['coarse_targets'][idx][1,:,:].numpy())\n",
    "ax[1,0].imshow(data_b['coarse_targets'][idx][2,:,:].numpy())\n",
    "ax[1,1].imshow(data_b['coarse_targets'][idx][3,:,:].numpy())\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(encoder_name=encoder_name,\n",
    "                 decoder_use_batchnorm=True,\n",
    "                 classes=output_channels,\n",
    "                 num_classes_classification=output_channels_class)\n",
    "loss_f_segmentation = get_loss('bce_dice')\n",
    "losses = dict({'loss_f_segmentation' : loss_f_segmentation, \n",
    "               'coarse_loss_f_segmentation' : loss_f_segmentation, \n",
    "                'loss_f_classification' : L.BinaryFocalLoss()\n",
    "              })\n",
    "optimizer = RAdam(model.parameters(), lr = 3e-4)\n",
    "optimizer_Lookahead = Lookahead(optimizer)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_Lookahead, \n",
    "                                                 milestones=[10, 15, 20],\n",
    "                                                 gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = collections.OrderedDict()\n",
    "loaders[\"train\"] = dataloader_train\n",
    "loaders[\"valid\"] = dataloader_val\n",
    "runner = SupervisedRunner(input_key = 'features',\n",
    "                          output_key =  None,\n",
    "                          input_target_key = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=losses,\n",
    "    optimizer=optimizer_Lookahead,\n",
    "    callbacks=[\n",
    "        CriterionCallback(input_key=\"targets\",\n",
    "                          output_key=\"logits\",\n",
    "                          prefix=\"loss_segmentation\",\n",
    "                          criterion_key='loss_f_segmentation', \n",
    "                          multiplier=1.0), \n",
    "        \n",
    "        CriterionCallback(input_key=\"coarse_targets\",\n",
    "                          output_key=\"coarse_logits\",\n",
    "                          prefix=\"coarse_loss_segmentation\",\n",
    "                          criterion_key='coarse_loss_f_segmentation',\n",
    "                          multiplier=1.0), \n",
    "        \n",
    "        CriterionCallback(input_key=\"classification_labels\",\n",
    "                          output_key=\"classification_logits\",\n",
    "                          prefix=\"classification_loss\",\n",
    "                          criterion_key='loss_f_classification',  \n",
    "                          multiplier=10.0), \n",
    "        \n",
    "        CriterionAggregatorCallback(prefix='combined_loss', \n",
    "                                    loss_keys=['loss_segmentation',\n",
    "                                               'coarse_loss_segmentation',\n",
    "                                               'classification_loss'\n",
    "                                              ],\n",
    "                                    loss_aggregate_fn='sum'),\n",
    "        \n",
    "        OptimizerCallback(accumulation_steps=accumulation_steps, \n",
    "                          loss_key='loss_segmentation'),\n",
    "        \n",
    "        IoUMetricsCallback(mode='multilabel',\n",
    "                           output_key=\"logits\",\n",
    "                           input_key='targets',\n",
    "                           metric=\"dice\",\n",
    "                           prefix='dice',\n",
    "                           nan_score_on_empty=False),\n",
    "        \n",
    "        IoUMetricsCallback(mode='multilabel',\n",
    "                           output_key=\"coarse_logits\",\n",
    "                           input_key='coarse_targets',\n",
    "                           metric=\"dice\",\n",
    "                           prefix='coarse_dice',\n",
    "                           nan_score_on_empty=False),\n",
    "        \n",
    "        DiceScoreCallback(mode='binary',\n",
    "                          output_key=\"logits\",\n",
    "                          input_key='targets',                          \n",
    "                          prefix='total_dice',\n",
    "                          nan_score_on_empty=False),\n",
    "        F1ScoreCallback(input_key='classification_labels',\n",
    "                        output_key='classification_logits',\n",
    "                        prefix='f1_score',),\n",
    "        AUCCallback(num_classes=output_channels_class,\n",
    "                    input_key='classification_labels',\n",
    "                    output_key='classification_logits')\n",
    "    ],\n",
    "    loaders=loaders,\n",
    "    logdir=log_dir,\n",
    "    main_metric='combined_loss',\n",
    "    num_epochs=num_epochs_with_frozen_encoder,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "model.load_state_dict(torch.load(os.path.join(log_dir,'checkpoints/best.pth'))['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=losses,\n",
    "    optimizer=optimizer_Lookahead,\n",
    "    callbacks=[\n",
    "        CriterionCallback(input_key=\"targets\",\n",
    "                          output_key=\"logits\",\n",
    "                          prefix=\"loss_segmentation\",\n",
    "                          criterion_key='loss_f_segmentation', \n",
    "                          multiplier=1.0), \n",
    "        \n",
    "        CriterionCallback(input_key=\"coarse_targets\",\n",
    "                          output_key=\"coarse_logits\",\n",
    "                          prefix=\"coarse_loss_segmentation\",\n",
    "                          criterion_key='coarse_loss_f_segmentation',\n",
    "                          multiplier=1.0), \n",
    "        \n",
    "        CriterionCallback(input_key=\"classification_labels\",\n",
    "                          output_key=\"classification_logits\",\n",
    "                          prefix=\"classification_loss\",\n",
    "                          criterion_key='loss_f_classification',  \n",
    "                          multiplier=10.0), \n",
    "        \n",
    "        CriterionAggregatorCallback(prefix='combined_loss', \n",
    "                                    loss_keys=['loss_segmentation',\n",
    "                                               'coarse_loss_segmentation',\n",
    "                                               'classification_loss'\n",
    "                                              ],\n",
    "                                    loss_aggregate_fn='sum'),\n",
    "        \n",
    "        OptimizerCallback(accumulation_steps=accumulation_steps, \n",
    "                          loss_key='loss_segmentation'),\n",
    "        \n",
    "        IoUMetricsCallback(mode='multilabel',\n",
    "                           output_key=\"logits\",\n",
    "                           input_key='targets',\n",
    "                           metric=\"dice\",\n",
    "                           prefix='dice',\n",
    "                           nan_score_on_empty=False),\n",
    "        \n",
    "        IoUMetricsCallback(mode='multilabel',\n",
    "                           output_key=\"coarse_logits\",\n",
    "                           input_key='coarse_targets',\n",
    "                           metric=\"dice\",\n",
    "                           prefix='coarse_dice',\n",
    "                           nan_score_on_empty=False),\n",
    "        \n",
    "        DiceScoreCallback(mode='binary',\n",
    "                          output_key=\"logits\",\n",
    "                          input_key='targets',                          \n",
    "                          prefix='total_dice',\n",
    "                          nan_score_on_empty=False),\n",
    "        F1ScoreCallback(input_key='classification_labels',\n",
    "                        output_key='classification_logits',\n",
    "                        prefix='f1_score',),\n",
    "        AUCCallback(num_classes=output_channels_class,\n",
    "                    input_key='classification_labels',\n",
    "                    output_key='classification_logits')\n",
    "    ],\n",
    "    loaders=loaders,\n",
    "    logdir=log_dir,\n",
    "    num_epochs=num_epochs,\n",
    "    main_metric='combined_loss',\n",
    "    #resume=f\"{log_dir}/checkpoints/best.pth\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = provider(\n",
    "    data_folder=data_folder,\n",
    "    df_path=train_df_path,\n",
    "    phase='train',\n",
    "    transforms=hard_augmentations(),\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers, \n",
    "    prepare_coarse=True, \n",
    "    prepare_edges=False, \n",
    "    prepare_class=True,\n",
    "    prepare_full=False)\n",
    "dataloader_val = provider(\n",
    "    data_folder=data_folder,\n",
    "    df_path=train_df_path,\n",
    "    phase='val',\n",
    "    transforms=validation_augmentations(),\n",
    "    batch_size=batch_size_val,\n",
    "    num_workers=num_workers, \n",
    "    prepare_coarse=True, \n",
    "    prepare_edges=False, \n",
    "    prepare_class=True,\n",
    "    prepare_full=False)\n",
    "loaders = collections.OrderedDict()\n",
    "loaders[\"train\"] = dataloader_train\n",
    "loaders[\"valid\"] = dataloader_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(log_dir,'checkpoints/best.pth'))['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=losses,\n",
    "    optimizer=optimizer_Lookahead,\n",
    "    callbacks=[\n",
    "        CriterionCallback(input_key=\"targets\",\n",
    "                          output_key=\"logits\",\n",
    "                          prefix=\"loss_segmentation\",\n",
    "                          criterion_key='loss_f_segmentation', \n",
    "                          multiplier=1.0), \n",
    "        \n",
    "        CriterionCallback(input_key=\"coarse_targets\",\n",
    "                          output_key=\"coarse_logits\",\n",
    "                          prefix=\"coarse_loss_segmentation\",\n",
    "                          criterion_key='coarse_loss_f_segmentation',\n",
    "                          multiplier=1.0), \n",
    "        \n",
    "        CriterionCallback(input_key=\"classification_labels\",\n",
    "                          output_key=\"classification_logits\",\n",
    "                          prefix=\"classification_loss\",\n",
    "                          criterion_key='loss_f_classification',  \n",
    "                          multiplier=10.0), \n",
    "        \n",
    "        CriterionAggregatorCallback(prefix='combined_loss', \n",
    "                                    loss_keys=['loss_segmentation',\n",
    "                                               'coarse_loss_segmentation',\n",
    "                                               'classification_loss'\n",
    "                                              ],\n",
    "                                    loss_aggregate_fn='sum'),\n",
    "        \n",
    "        OptimizerCallback(accumulation_steps=accumulation_steps, \n",
    "                          loss_key='loss_segmentation'),\n",
    "        \n",
    "        IoUMetricsCallback(mode='multilabel',\n",
    "                           output_key=\"logits\",\n",
    "                           input_key='targets',\n",
    "                           metric=\"dice\",\n",
    "                           prefix='dice',\n",
    "                           nan_score_on_empty=False),\n",
    "        \n",
    "        IoUMetricsCallback(mode='multilabel',\n",
    "                           output_key=\"coarse_logits\",\n",
    "                           input_key='coarse_targets',\n",
    "                           metric=\"dice\",\n",
    "                           prefix='coarse_dice',\n",
    "                           nan_score_on_empty=False),\n",
    "        \n",
    "        DiceScoreCallback(mode='binary',\n",
    "                          output_key=\"logits\",\n",
    "                          input_key='targets',                          \n",
    "                          prefix='total_dice',\n",
    "                          nan_score_on_empty=False),\n",
    "        F1ScoreCallback(input_key='classification_labels',\n",
    "                        output_key='classification_logits',\n",
    "                        prefix='f1_score',),\n",
    "        AUCCallback(num_classes=output_channels_class,\n",
    "                    input_key='classification_labels',\n",
    "                    output_key='classification_logits')\n",
    "    ],\n",
    "    loaders=loaders,\n",
    "    logdir=log_dir,\n",
    "    num_epochs=num_epochs,\n",
    "    main_metric='combined_loss',\n",
    "    resume=f\"{log_dir}/checkpoints/best.pth\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
